"""
è…¾è®¯è‚¡ç¥¨ AI Agent - åç«¯æœåŠ¡
æä¾›æ–°é—»æŠ“å–ã€è‚¡ç¥¨æ•°æ®è·å–ã€AIåˆ†æç­‰API
"""

import os
import json
import asyncio
import time
from datetime import datetime, timedelta
from typing import Optional

import httpx
from bs4 import BeautifulSoup
from fastapi import FastAPI, HTTPException
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, StreamingResponse
from pydantic import BaseModel
from pathlib import Path
from dotenv import load_dotenv

# ç¡®ä¿ä» app.py æ‰€åœ¨ç›®å½•åŠ è½½ .env
_APP_DIR = Path(__file__).resolve().parent
_env_file = _APP_DIR / ".env"
load_dotenv(_env_file)

app = FastAPI(title="è…¾è®¯è‚¡ç¥¨AI Agent")

# ---------------------------------------------------------------------------
# é…ç½®
# ---------------------------------------------------------------------------
LLM_API_KEY = os.getenv("LLM_API_KEY", "")
LLM_BASE_URL = os.getenv("LLM_BASE_URL", "https://api.openai.com/v1")
LLM_MODEL = os.getenv("LLM_MODEL", "gpt-4o-mini")


@app.on_event("startup")
async def startup_log():
    import logging
    logging.warning(
        f"[Config] env={_env_file} exists={_env_file.exists()} "
        f"LLM_KEY={'SET(' + str(len(LLM_API_KEY)) + ')' if LLM_API_KEY else 'EMPTY'} "
        f"MODEL={LLM_MODEL}"
    )

# ç®€å•å†…å­˜ç¼“å­˜
_cache: dict = {}
CACHE_TTL = 300  # 5åˆ†é’Ÿ

# AI è¯„çº§æ¯æ—¥ç¼“å­˜ï¼ˆkey: æ—¥æœŸå­—ç¬¦ä¸², value: è¯„çº§ç»“æœ dictï¼‰
_rating_cache: dict = {}


def _get_cache(key: str):
    if key in _cache:
        ts, data = _cache[key]
        if time.time() - ts < CACHE_TTL:
            return data
    return None


def _set_cache(key: str, data):
    _cache[key] = (time.time(), data)


# ---------------------------------------------------------------------------
# è‚¡ç¥¨æ•°æ®è·å– (ä½¿ç”¨å…¬å¼€æ¥å£)
# ---------------------------------------------------------------------------
async def fetch_stock_data() -> dict:
    """è·å–è…¾è®¯æ§è‚¡(00700.HK)å®æ—¶è‚¡ç¥¨æ•°æ®"""
    cached = _get_cache("stock_data")
    if cached:
        return cached

    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                       "AppleWebKit/537.36 (KHTML, like Gecko) "
                       "Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://finance.sina.com.cn",
    }

    stock_info = {
        "name": "è…¾è®¯æ§è‚¡",
        "code": "00700.HK",
        "current_price": "--",
        "change": "--",
        "change_percent": "--",
        "open": "--",
        "high": "--",
        "low": "--",
        "prev_close": "--",
        "volume": "--",
        "turnover": "--",
        "market_cap": "--",
        "pe_ratio": "--",
        "52w_high": "--",
        "52w_low": "--",
        "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    }

    try:
        async with httpx.AsyncClient(timeout=10, headers=headers) as client:
            # æ–°æµªæ¸¯è‚¡å®æ—¶æ•°æ®æ¥å£
            resp = await client.get(
                "https://hq.sinajs.cn/list=rt_hk00700",
                headers={**headers, "Referer": "https://finance.sina.com.cn"},
            )
            if resp.status_code == 200:
                text = resp.text
                # æ ¼å¼: var hq_str_rt_hk00700="...å­—æ®µç”¨é€—å·åˆ†éš”..."
                if '"' in text:
                    data_str = text.split('"')[1]
                    fields = data_str.split(",")
                    if len(fields) > 15:
                        stock_info.update({
                            "name": "è…¾è®¯æ§è‚¡",
                            "name_en": fields[0] if fields[0] else "TENCENT",
                            "current_price": fields[6],
                            "change": fields[7],
                            "change_percent": fields[8],
                            "prev_close": fields[3],
                            "open": fields[2],
                            "high": fields[4],
                            "low": fields[5],
                            "volume": fields[12],
                            "turnover": fields[11],
                            "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        })
    except Exception as e:
        print(f"[Stock] æ–°æµªæ¥å£å¼‚å¸¸: {e}")

    # å°è¯•è…¾è®¯è´¢ç»æ¥å£è·å–æ›´å¤šæ•°æ®
    try:
        from datetime import date
        today = date.today().strftime("%Y-%m-%d")
        async with httpx.AsyncClient(timeout=10, headers=headers) as client:
            resp = await client.get(
                f"https://web.ifzq.gtimg.cn/appstock/app/fqkline/get"
                f"?param=hk00700,day,,{today},5,qfq"
            )
            if resp.status_code == 200:
                data = resp.json()
                if "data" in data and "hk00700" in data["data"]:
                    qt = data["data"]["hk00700"].get("qt", {}).get("hk00700", [])
                    if qt and len(qt) > 45:
                        if stock_info["current_price"] == "--":
                            stock_info["current_price"] = qt[3]
                        stock_info["market_cap"] = qt[45] if len(qt) > 45 else "--"
                        stock_info["pe_ratio"] = qt[39] if len(qt) > 39 else "--"
                        stock_info["52w_high"] = qt[48] if len(qt) > 48 else "--"
                        stock_info["52w_low"] = qt[49] if len(qt) > 49 else "--"
                        stock_info["dividend_yield"] = qt[47] if len(qt) > 47 else "--"
                        stock_info["pb_ratio"] = qt[51] if len(qt) > 51 else "--"
                        stock_info["turnover_rate"] = qt[50] if len(qt) > 50 else "--"
                        stock_info["amplitude"] = qt[43] if len(qt) > 43 else "--"
                        stock_info["total_shares"] = qt[69] if len(qt) > 69 else "--"
                        stock_info["float_shares"] = qt[70] if len(qt) > 70 else "--"
                        stock_info["nav_per_share"] = qt[72] if len(qt) > 72 else "--"
    except Exception as e:
        print(f"[Stock] è…¾è®¯æ¥å£å¼‚å¸¸: {e}")

    _set_cache("stock_data", stock_info)
    return stock_info


async def fetch_kline_data(period: str = "day", count: int = 60) -> list:
    """è·å–è…¾è®¯Kçº¿æ•°æ®
    period: day(æ—¥çº¿), week(å‘¨çº¿), month(æœˆçº¿)
    count: è¯·æ±‚æ•°æ®æ¡æ•°
    """
    cache_key = f"kline_{period}_{count}"
    cached = _get_cache(cache_key)
    if cached:
        return cached

    # é™åˆ¶åˆæ³•å€¼
    if period not in ("day", "week", "month"):
        period = "day"
    count = min(max(count, 10), 1500)

    kline_list = []
    try:
        from datetime import date
        today = date.today().strftime("%Y-%m-%d")
        async with httpx.AsyncClient(timeout=15) as client:
            resp = await client.get(
                f"https://web.ifzq.gtimg.cn/appstock/app/fqkline/get"
                f"?param=hk00700,{period},,{today},{count},qfq"
            )
            if resp.status_code == 200:
                raw = resp.json()
                d = raw.get("data", {})
                if isinstance(d, dict):
                    hk = d.get("hk00700", {})
                    if isinstance(hk, dict):
                        items = (
                            hk.get(period, [])
                            or hk.get(f"qfq{period}", [])
                        )
                        for row in items:
                            kline_list.append({
                                "date": row[0],
                                "open": float(row[1]),
                                "close": float(row[2]),
                                "high": float(row[3]),
                                "low": float(row[4]),
                                "volume": float(row[5]) if len(row) > 5 else 0,
                            })
    except Exception as e:
        print(f"[KLine] è·å–Kçº¿å¼‚å¸¸({period}/{count}): {e}")

    _set_cache(cache_key, kline_list)
    return kline_list


# ---------------------------------------------------------------------------
# æ–°é—»æŠ“å– (å¤šç»´åº¦ä¸“ä¸šè‚¡ç¥¨èµ„è®¯)
# ---------------------------------------------------------------------------

# è‚¡ç¥¨ä¸“ä¸šå…³é”®è¯ï¼Œç”¨äºæ ‡è®°æ–°é—»ç±»å‹
_STOCK_KEYWORDS = [
    "è‚¡ä»·", "æ¸¯è‚¡", "æ¶¨", "è·Œ", "å¸‚å€¼", "è´¢æŠ¥", "è¥æ”¶", "å‡€åˆ©", "åˆ©æ¶¦",
    "ç ”æŠ¥", "è¯„çº§", "ç›®æ ‡ä»·", "å›è´­", "åˆ†çº¢", "æ´¾æ¯", "å¢æŒ", "å‡æŒ",
    "å¤§è¡Œ", "åˆ¸å•†", "åˆ†æå¸ˆ", "æŠ•è¡Œ", "æ‘©æ ¹", "é«˜ç››", "ç‘é“¶", "èŠ±æ——",
    "ç¾é“¶", "æ±‡ä¸°", "æ‘©é€š", "å¤§æ‘©", "å°æ‘©", "ä¸­é‡‘", "ä¸­ä¿¡", "åæ³°",
    "ä¼°å€¼", "PE", "å¸‚ç›ˆç‡", "EPS", "ä¸šç»©", "å­£æŠ¥", "å¹´æŠ¥", "ä¸­æŠ¥",
    "Kçº¿", "å‡çº¿", "æŠ€æœ¯é¢", "åŸºæœ¬é¢", "åšå¤š", "åšç©º", "èèµ„", "èåˆ¸",
    "æ’ç”Ÿ", "æ’æŒ‡", "ç§‘æŠ€è‚¡", "ä¸­æ¦‚è‚¡", "ADR", "æˆäº¤é¢", "æ¢æ‰‹ç‡",
    "ç‰›å¸‚", "ç†Šå¸‚", "åå¼¹", "å›è°ƒ", "çªç ´", "æ”¯æ’‘", "é˜»åŠ›",
    "00700", "HK", "æ§è‚¡",
    # English keywords for international media
    "stock", "share", "rally", "surge", "drop", "plunge", "dividend",
    "earnings", "revenue", "profit", "valuation", "analyst", "upgrade",
    "downgrade", "target price", "buyback", "IPO", "HKEX",
    "Morgan", "Goldman", "UBS", "Citi", "HSBC", "JP Morgan",
    "bull", "bear", "rally", "sell-off",
]


def _classify_news(title: str, summary: str = "") -> str:
    """æ ¹æ®æ ‡é¢˜å’Œæ‘˜è¦åˆ¤æ–­æ–°é—»ç±»å‹: stock(è‚¡ç¥¨ä¸“ä¸š) / general(ç»¼åˆèµ„è®¯)"""
    text = title + summary
    for kw in _STOCK_KEYWORDS:
        if kw in text:
            return "stock"
    return "general"


async def _search_sina(client: httpx.AsyncClient, query: str, num: int = 15) -> list:
    """é€šç”¨æ–°æµªæœç´¢æ–¹æ³•"""
    results = []
    try:
        resp = await client.get(
            "https://search.sina.com.cn/news",
            params={
                "q": query,
                "c": "news",
                "from": "channel",
                "ie": "utf-8",
                "num": str(num),
            },
        )
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.text, "lxml")
            for item in soup.select(".box-result"):
                h2 = item.select_one("h2 a")
                if h2:
                    title = h2.get_text(strip=True)
                    url = h2.get("href", "")
                    info = item.select_one(".fgray_time")
                    time_str = info.get_text(strip=True) if info else ""
                    summary_el = item.select_one(".content")
                    summary = summary_el.get_text(strip=True)[:120] if summary_el else ""
                    if title:
                        tag = _classify_news(title, summary)
                        results.append({
                            "title": title,
                            "url": url,
                            "source": "æ–°æµªè´¢ç»",
                            "time": time_str,
                            "summary": summary,
                            "tag": tag,
                        })
    except Exception as e:
        print(f"[News] æ–°æµªæœç´¢({query})å¼‚å¸¸: {e}")
    return results


async def fetch_news() -> list:
    """å¤šç»´åº¦æŠ“å–è…¾è®¯æ§è‚¡ä¸“ä¸šè‚¡ç¥¨èµ„è®¯"""
    cached = _get_cache("news_data")
    if cached:
        return cached

    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                       "AppleWebKit/537.36 (KHTML, like Gecko) "
                       "Chrome/120.0.0.0 Safari/537.36",
    }

    all_news: list = []

    async with httpx.AsyncClient(timeout=15, headers=headers, follow_redirects=True) as client:
        # å¹¶å‘å‘èµ·å¤šç»´åº¦æœç´¢
        tasks = [
            _search_sina(client, "è…¾è®¯æ§è‚¡ è‚¡ä»·", 10),
            _search_sina(client, "è…¾è®¯ æ¸¯è‚¡ åˆ†æ", 10),
            _search_sina(client, "00700 ç ”æŠ¥", 8),
            _search_sina(client, "è…¾è®¯", 10),
        ]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        for res in results:
            if isinstance(res, list):
                all_news.extend(res)

    # è¡¥å……æ¥æº: æ–°æµªè´¢ç»æ»šåŠ¨æ–°é—»
    try:
        async with httpx.AsyncClient(timeout=10, headers=headers) as client:
            resp = await client.get(
                "https://feed.mix.sina.com.cn/api/roll/get",
                params={
                    "pageid": "153",
                    "lid": "2516",
                    "k": "è…¾è®¯",
                    "num": "20",
                    "page": "1",
                },
            )
            if resp.status_code == 200:
                data = resp.json()
                items = data.get("result", {}).get("data", [])
                for item in items:
                    title = item.get("title", "").strip()
                    intro = item.get("intro", "")
                    if title and ("è…¾è®¯" in title or "è…¾è®¯" in intro):
                        tag = _classify_news(title, intro)
                        all_news.append({
                            "title": title,
                            "url": item.get("url", ""),
                            "source": item.get("media_name", "æ–°æµªè´¢ç»") or "æ–°æµªè´¢ç»",
                            "time": datetime.fromtimestamp(
                                int(item.get("ctime", 0))
                            ).strftime("%m-%d %H:%M") if item.get("ctime") else "",
                            "summary": intro[:120] if intro else "",
                            "tag": tag,
                        })
    except Exception as e:
        print(f"[News] æ–°æµªè´¢ç»æ»šåŠ¨å¼‚å¸¸: {e}")

    # æ¥æº: Google News RSS (å›½é™…åª’ä½“è‹±æ–‡æ–°é—»)
    for gn_query in ["Tencent+stock", "Tencent+00700"]:
        try:
            async with httpx.AsyncClient(timeout=12, headers=headers, follow_redirects=True) as client:
                resp = await client.get(
                    f"https://news.google.com/rss/search?q={gn_query}&hl=en&gl=US&ceid=US:en"
                )
                if resp.status_code == 200:
                    soup = BeautifulSoup(resp.text, "xml")
                    for item in soup.select("item")[:8]:
                        title = item.select_one("title")
                        link = item.select_one("link")
                        source_el = item.select_one("source")
                        pub_date = item.select_one("pubDate")
                        if title and link:
                            t = title.get_text(strip=True)
                            s = source_el.get_text(strip=True) if source_el else "Google News"
                            time_str = ""
                            if pub_date:
                                try:
                                    from email.utils import parsedate_to_datetime
                                    dt = parsedate_to_datetime(pub_date.get_text(strip=True))
                                    time_str = dt.strftime("%m-%d %H:%M")
                                except Exception:
                                    pass
                            tag = _classify_news(t, "")
                            all_news.append({
                                "title": t,
                                "url": link.get_text(strip=True),
                                "source": s,
                                "time": time_str,
                                "summary": "",
                                "tag": tag,
                                "lang": "en",
                            })
        except Exception as e:
            print(f"[News] Google News({gn_query})å¼‚å¸¸: {e}")

    # å»é‡ï¼ˆæŒ‰æ ‡é¢˜ï¼‰
    seen: set = set()
    unique_news: list = []
    for n in all_news:
        if n["title"] not in seen:
            seen.add(n["title"])
            unique_news.append(n)

    # æ’åºï¼šè‚¡ç¥¨ä¸“ä¸šç±»ä¼˜å…ˆ
    unique_news.sort(key=lambda x: (0 if x.get("tag") == "stock" else 1))

    result = unique_news[:25]
    _set_cache("news_data", result)
    return result


# ---------------------------------------------------------------------------
# AI åˆ†æ (æµå¼)
# ---------------------------------------------------------------------------
async def stream_ai_analysis(stock_data: dict, news_list: list, kline_data: list):
    """è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆæµå¼åˆ†ææŠ¥å‘Š"""
    # æ„å»ºåˆ†æä¸Šä¸‹æ–‡
    news_text = "\n".join(
        [f"- {n['title']}ï¼ˆ{n['source']}ï¼‰" for n in news_list[:10]]
    ) or "æš‚æ— æœ€æ–°æ–°é—»"

    kline_summary = ""
    if kline_data:
        recent = kline_data[-5:]
        kline_summary = "è¿‘5ä¸ªäº¤æ˜“æ—¥è¡Œæƒ…:\n"
        for k in recent:
            kline_summary += (
                f"  {k['date']}: å¼€{k['open']} æ”¶{k['close']} "
                f"é«˜{k['high']} ä½{k['low']}\n"
            )

    prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±æ¸¯è‚¡åˆ†æå¸ˆå’ŒAIæŠ•èµ„é¡¾é—®ã€‚è¯·æ ¹æ®ä»¥ä¸‹è…¾è®¯æ§è‚¡(00700.HK)çš„æœ€æ–°æ•°æ®ï¼Œ
ç»™å‡ºä¸“ä¸šã€è¯¦ç»†çš„è‚¡ç¥¨åˆ†ææŠ¥å‘Šå’ŒæŠ•èµ„å»ºè®®ã€‚

## å½“å‰è‚¡ç¥¨æ•°æ®
- è‚¡ç¥¨åç§°: {stock_data.get('name', 'è…¾è®¯æ§è‚¡')}
- è‚¡ç¥¨ä»£ç : {stock_data.get('code', '00700.HK')}
- å½“å‰ä»·æ ¼: {stock_data.get('current_price', '--')} HKD
- æ¶¨è·Œé¢: {stock_data.get('change', '--')}
- æ¶¨è·Œå¹…: {stock_data.get('change_percent', '--')}%
- ä»Šå¼€: {stock_data.get('open', '--')}
- æœ€é«˜: {stock_data.get('high', '--')}
- æœ€ä½: {stock_data.get('low', '--')}
- æ˜¨æ”¶: {stock_data.get('prev_close', '--')}
- æˆäº¤é‡: {stock_data.get('volume', '--')}
- æˆäº¤é¢: {stock_data.get('turnover', '--')}
- å¸‚å€¼: {stock_data.get('market_cap', '--')}
- å¸‚ç›ˆç‡: {stock_data.get('pe_ratio', '--')}
- 52å‘¨æœ€é«˜: {stock_data.get('52w_high', '--')}
- 52å‘¨æœ€ä½: {stock_data.get('52w_low', '--')}

{kline_summary}

## æœ€æ–°ç›¸å…³æ–°é—»
{news_text}

## è¯·è¾“å‡ºä»¥ä¸‹å†…å®¹:
1. **å¸‚åœºæ¦‚è§ˆ** - å½“å‰ä»·æ ¼èµ°åŠ¿åˆ†æ
2. **æŠ€æœ¯é¢åˆ†æ** - åŸºäºKçº¿æ•°æ®çš„æŠ€æœ¯æŒ‡æ ‡åˆ†æ
3. **æ¶ˆæ¯é¢åˆ†æ** - æ ¹æ®æœ€æ–°æ–°é—»è§£è¯»å¸‚åœºæƒ…ç»ª
4. **åŸºæœ¬é¢åˆ†æ** - ä¼°å€¼æ°´å¹³å’Œä¸šåŠ¡å‘å±•
5. **é£é™©æç¤º** - å½“å‰é¢ä¸´çš„ä¸»è¦é£é™©
6. **æ“ä½œå»ºè®®** - ç»™å‡ºå…·ä½“çš„æŠ•èµ„å»ºè®®ï¼ˆçŸ­æœŸ/ä¸­æœŸ/é•¿æœŸï¼‰

è¯·ä½¿ç”¨Markdownæ ¼å¼è¾“å‡ºï¼Œè¦æ±‚ä¸“ä¸šã€å®¢è§‚ã€å…¨é¢ã€‚åˆ†ææ—¥æœŸ: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}

âš ï¸ å…è´£å£°æ˜ï¼šä»¥ä¸Šåˆ†æä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚æŠ•èµ„æœ‰é£é™©ï¼Œå…¥å¸‚éœ€è°¨æ…ã€‚"""

    if not LLM_API_KEY:
        # æ²¡æœ‰é…ç½®API Keyæ—¶ï¼Œè¿”å›æ¨¡æ‹Ÿåˆ†æ
        yield _generate_fallback_analysis(stock_data, news_list, kline_data)
        return

    try:
        req_body = {
            "model": LLM_MODEL,
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±æ¸¯è‚¡åˆ†æå¸ˆï¼Œæ“…é•¿æŠ€æœ¯åˆ†æå’ŒåŸºæœ¬é¢åˆ†æã€‚ä½ çš„åˆ†æä¸“ä¸šã€å®¢è§‚ã€å…¨é¢ã€‚"},
                {"role": "user", "content": prompt},
            ],
            "stream": True,
            "temperature": 0.7,
            "max_tokens": 3000,
        }
        # Gemini 2.5 thinking æ¨¡å‹: ç”¨ low é™åˆ¶æ€è€ƒ tokenï¼ŒæŠŠæ›´å¤šé…é¢ç»™å®é™…è¾“å‡º
        if "2.5" in LLM_MODEL:
            req_body["max_tokens"] = 8000
            req_body["reasoning_effort"] = "low"

        async with httpx.AsyncClient(timeout=httpx.Timeout(300, connect=15)) as client:
            async with client.stream(
                "POST",
                f"{LLM_BASE_URL}/chat/completions",
                headers={
                    "Authorization": f"Bearer {LLM_API_KEY}",
                    "Content-Type": "application/json",
                },
                json=req_body,
            ) as response:
                if response.status_code != 200:
                    body = await response.aread()
                    error_msg = ""
                    try:
                        err = json.loads(body)
                        if isinstance(err, list) and err:
                            err = err[0]
                        error_msg = err.get("error", {}).get("message", "")[:200]
                    except Exception:
                        error_msg = body.decode("utf-8", errors="ignore")[:200]
                    print(f"[AI] APIè¿”å› {response.status_code}: {error_msg}")
                    yield f"\n\n> âš ï¸ **AIæ¨¡å‹è°ƒç”¨å¤±è´¥**ï¼ˆHTTP {response.status_code}ï¼‰ï¼š{error_msg}\n\n> å·²é™çº§ä¸ºæœ¬åœ°æ¨¡æ¿åˆ†æã€‚\n\n---\n\n"
                    yield _generate_fallback_analysis(stock_data, news_list, kline_data)
                    return

                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data_str = line[6:]
                        if data_str.strip() == "[DONE]":
                            break
                        try:
                            chunk = json.loads(data_str)
                            delta = chunk["choices"][0]["delta"]
                            if "content" in delta:
                                yield delta["content"]
                        except (json.JSONDecodeError, KeyError, IndexError):
                            continue
    except Exception as e:
        print(f"[AI] è°ƒç”¨å¼‚å¸¸: {e}")
        yield _generate_fallback_analysis(stock_data, news_list, kline_data)


def _generate_fallback_analysis(stock_data: dict, news_list: list, kline_data: list) -> str:
    """å½“AIæ¥å£ä¸å¯ç”¨æ—¶çš„æœ¬åœ°åˆ†ææŠ¥å‘Š"""
    price = stock_data.get("current_price", "--")
    change = stock_data.get("change", "--")
    change_pct = stock_data.get("change_percent", "--")
    now = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")

    # åˆ†æKçº¿è¶‹åŠ¿
    trend_text = ""
    if kline_data and len(kline_data) >= 5:
        recent_5 = kline_data[-5:]
        closes = [k["close"] for k in recent_5]
        if closes[-1] > closes[0]:
            trend_text = "è¿‘5ä¸ªäº¤æ˜“æ—¥æ•´ä½“å‘ˆä¸Šæ¶¨è¶‹åŠ¿"
        elif closes[-1] < closes[0]:
            trend_text = "è¿‘5ä¸ªäº¤æ˜“æ—¥æ•´ä½“å‘ˆä¸‹è·Œè¶‹åŠ¿"
        else:
            trend_text = "è¿‘5ä¸ªäº¤æ˜“æ—¥æ•´ä½“å‘ˆéœ‡è¡æ€åŠ¿"

        avg_5 = sum(closes) / len(closes)
        if len(kline_data) >= 20:
            avg_20 = sum(k["close"] for k in kline_data[-20:]) / 20
            if avg_5 > avg_20:
                trend_text += "ï¼Œ5æ—¥å‡çº¿ä½äº20æ—¥å‡çº¿ä¸Šæ–¹ï¼ŒçŸ­æœŸåå¤š"
            else:
                trend_text += "ï¼Œ5æ—¥å‡çº¿ä½äº20æ—¥å‡çº¿ä¸‹æ–¹ï¼ŒçŸ­æœŸåç©º"

    news_section = ""
    if news_list:
        news_items = "\n".join([f"- {n['title']}ï¼ˆæ¥æº: {n['source']}ï¼‰" for n in news_list[:8]])
        news_section = f"""
### ğŸ“° æœ€æ–°æ–°é—»åŠ¨æ€

{news_items}

ä»¥ä¸Šæ–°é—»åæ˜ äº†å¸‚åœºå¯¹è…¾è®¯çš„æœ€æ–°å…³æ³¨ç‚¹ã€‚æŠ•èµ„è€…éœ€ç»“åˆæ–°é—»å†…å®¹åˆ†æå¯¹è‚¡ä»·çš„æ½œåœ¨å½±å“ã€‚
"""
    else:
        news_section = "\n### ğŸ“° æœ€æ–°æ–°é—»åŠ¨æ€\n\næš‚æœªè·å–åˆ°æœ€æ–°çš„è…¾è®¯ç›¸å…³æ–°é—»ã€‚\n"

    kline_detail = ""
    if kline_data and len(kline_data) >= 5:
        kline_rows = ""
        for k in kline_data[-5:]:
            change_val = k["close"] - k["open"]
            emoji = "ğŸ”´" if change_val >= 0 else "ğŸŸ¢"
            kline_rows += f"| {k['date']} | {k['open']} | {k['close']} | {k['high']} | {k['low']} | {emoji} {change_val:+.2f} |\n"
        kline_detail = f"""
| æ—¥æœŸ | å¼€ç›˜ | æ”¶ç›˜ | æœ€é«˜ | æœ€ä½ | æ¶¨è·Œ |
|------|------|------|------|------|------|
{kline_rows}"""

    return f"""# ğŸ¦ è…¾è®¯æ§è‚¡ï¼ˆ00700.HKï¼‰AIåˆ†ææŠ¥å‘Š

> ğŸ“… åˆ†ææ—¶é—´ï¼š{now}

---

## 1. ğŸ“Š å¸‚åœºæ¦‚è§ˆ

è…¾è®¯æ§è‚¡ï¼ˆ00700.HKï¼‰å½“å‰æŠ¥ä»· **{price} HKD**ï¼Œæ¶¨è·Œé¢ {change}ï¼Œæ¶¨è·Œå¹… {change_pct}%ã€‚

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| å½“å‰ä»·æ ¼ | {price} HKD |
| ä»Šæ—¥å¼€ç›˜ | {stock_data.get('open', '--')} |
| æœ€é«˜ä»· | {stock_data.get('high', '--')} |
| æœ€ä½ä»· | {stock_data.get('low', '--')} |
| æ˜¨æ—¥æ”¶ç›˜ | {stock_data.get('prev_close', '--')} |
| æˆäº¤é‡ | {stock_data.get('volume', '--')} |
| æˆäº¤é¢ | {stock_data.get('turnover', '--')} |
| å¸‚ç›ˆç‡(PE) | {stock_data.get('pe_ratio', '--')} |
| æ€»å¸‚å€¼ | {stock_data.get('market_cap', '--')} |
| 52å‘¨æœ€é«˜ | {stock_data.get('52w_high', '--')} |
| 52å‘¨æœ€ä½ | {stock_data.get('52w_low', '--')} |

---

## 2. ğŸ“ˆ æŠ€æœ¯é¢åˆ†æ

{trend_text if trend_text else "æš‚æ— è¶³å¤Ÿçš„Kçº¿æ•°æ®è¿›è¡ŒæŠ€æœ¯åˆ†æã€‚"}

{kline_detail}

**æŠ€æœ¯æŒ‡æ ‡è§£è¯»ï¼š**
- å…³æ³¨æˆäº¤é‡å˜åŒ–ï¼Œæ”¾é‡ä¸Šæ¶¨ä¸ºç§¯æä¿¡å·
- å…³æ³¨å…³é”®æ”¯æ’‘ä½ä¸é˜»åŠ›ä½çš„çªç ´æƒ…å†µ
- å»ºè®®ç»“åˆMACDã€RSIç­‰æŠ€æœ¯æŒ‡æ ‡ç»¼åˆåˆ¤æ–­

---

## 3. ğŸ“° æ¶ˆæ¯é¢åˆ†æ

{news_section}

---

## 4. ğŸ¢ åŸºæœ¬é¢åˆ†æ

è…¾è®¯æ§è‚¡ä½œä¸ºä¸­å›½æœ€å¤§çš„äº’è”ç½‘å…¬å¸ä¹‹ä¸€ï¼Œä¸šåŠ¡æ¶µç›–ï¼š
- **æ¸¸æˆä¸šåŠ¡**ï¼šå…¨çƒé¢†å…ˆçš„æ¸¸æˆå‘è¡Œå•†ï¼ŒæŒç»­è´¡çŒ®æ ¸å¿ƒæ”¶å…¥
- **ç¤¾äº¤å¹³å°**ï¼šå¾®ä¿¡/WeChatæœˆæ´»è¶…13äº¿ï¼Œå…·å¤‡å¼ºå¤§çš„ç”Ÿæ€å£å’
- **é‡‘èç§‘æŠ€**ï¼šå¾®ä¿¡æ”¯ä»˜ã€ç†è´¢é€šç­‰é‡‘èç§‘æŠ€æœåŠ¡æŒç»­å¢é•¿
- **äº‘æœåŠ¡**ï¼šè…¾è®¯äº‘åœ¨å›½å†…å¸‚åœºä»½é¢ç¨³æ­¥æå‡
- **æŠ•èµ„ç”Ÿæ€**ï¼šæŒæœ‰ä¼—å¤šä¼˜è´¨å…¬å¸è‚¡æƒï¼ŒæŠ•èµ„å›æŠ¥å¯è§‚

å½“å‰å¸‚ç›ˆç‡ä¸º {stock_data.get('pe_ratio', '--')}ï¼ŒæŠ•èµ„è€…å¯å‚è€ƒå†å²ä¼°å€¼ä¸­æ¢è¯„ä¼°å½“å‰ä¼°å€¼æ°´å¹³ã€‚

---

## 5. âš ï¸ é£é™©æç¤º

1. **æ”¿ç­–ç›‘ç®¡é£é™©**ï¼šäº’è”ç½‘è¡Œä¸šæ”¿ç­–æŒç»­æ¼”å˜ï¼Œéœ€å…³æ³¨ç›‘ç®¡åŠ¨æ€
2. **å®è§‚ç»æµé£é™©**ï¼šå…¨çƒç»æµä¸ç¡®å®šæ€§å¯èƒ½å½±å“ä¸šåŠ¡å¢é•¿
3. **è¡Œä¸šç«äº‰é£é™©**ï¼šçŸ­è§†é¢‘ã€ç”µå•†ç­‰é¢†åŸŸç«äº‰åŠ å‰§
4. **åœ°ç¼˜æ”¿æ²»é£é™©**ï¼šä¸­ç¾å…³ç³»å˜åŒ–å¯èƒ½å½±å“æ¸¯è‚¡å¸‚åœºæƒ…ç»ª
5. **æ±‡ç‡é£é™©**ï¼šæ¸¯å¸å…‘äººæ°‘å¸æ±‡ç‡æ³¢åŠ¨å½±å“å®é™…æ”¶ç›Š

---

## 6. ğŸ’¡ æ“ä½œå»ºè®®

| ç­–ç•¥ | å»ºè®® |
|------|------|
| **çŸ­æœŸï¼ˆ1-2å‘¨ï¼‰** | å…³æ³¨æŠ€æœ¯é¢æ”¯æ’‘/å‹åŠ›ä½ï¼Œè½»ä»“çµæ´»æ“ä½œ |
| **ä¸­æœŸï¼ˆ1-3æœˆï¼‰** | å…³æ³¨è´¢æŠ¥å‘å¸ƒå’Œä¸šåŠ¡æ•°æ®ï¼Œé€¢ä½å¸ƒå±€ |
| **é•¿æœŸï¼ˆ6æœˆä»¥ä¸Šï¼‰** | è…¾è®¯åŸºæœ¬é¢ä¼˜è´¨ï¼Œé€‚åˆé•¿æœŸä»·å€¼æŠ•èµ„ |

---

> âš ï¸ **å…è´£å£°æ˜**ï¼šä»¥ä¸Šåˆ†æç”±AIç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆä»»ä½•æŠ•èµ„å»ºè®®ã€‚æŠ•èµ„æœ‰é£é™©ï¼Œå…¥å¸‚éœ€è°¨æ…ã€‚è¯·æŠ•èµ„è€…æ ¹æ®è‡ªèº«é£é™©æ‰¿å—èƒ½åŠ›åšå‡ºç‹¬ç«‹åˆ¤æ–­ã€‚
"""


# ---------------------------------------------------------------------------
# API è·¯ç”±
# ---------------------------------------------------------------------------
@app.get("/api/stock")
async def get_stock():
    """è·å–è…¾è®¯è‚¡ç¥¨å®æ—¶æ•°æ®"""
    data = await fetch_stock_data()
    return {"code": 0, "data": data}


@app.get("/api/kline")
async def get_kline(period: str = "day", count: int = 60):
    """è·å–Kçº¿æ•°æ®  period=day|week|month  count=æ•°æ®æ¡æ•°"""
    data = await fetch_kline_data(period=period, count=count)
    return {"code": 0, "data": data}


@app.get("/api/news")
async def get_news():
    """è·å–è…¾è®¯ç›¸å…³æ–°é—»"""
    data = await fetch_news()
    return {"code": 0, "data": data}


@app.get("/api/analysis")
async def get_analysis():
    """è·å–AIåˆ†ææŠ¥å‘Š(æµå¼)"""
    stock_data, news_list, kline_data = await asyncio.gather(
        fetch_stock_data(),
        fetch_news(),
        fetch_kline_data(),
    )

    async def _gen():
        async for chunk in stream_ai_analysis(stock_data, news_list, kline_data):
            yield chunk

    return _sse_wrap(_gen())


@app.post("/api/refresh")
async def refresh_data():
    """åˆ·æ–°ç¼“å­˜ï¼Œé‡æ–°è·å–æ•°æ®"""
    _cache.clear()
    _rating_cache.clear()
    stock_data, news_list, kline_data = await asyncio.gather(
        fetch_stock_data(),
        fetch_news(),
        fetch_kline_data(),
    )
    return {
        "code": 0,
        "data": {
            "stock": stock_data,
            "news": news_list,
            "kline_count": len(kline_data),
        },
    }


# ---------------------------------------------------------------------------
# AI æ¯æ—¥è¯„çº§
# ---------------------------------------------------------------------------
@app.get("/api/rating")
async def get_rating():
    """è·å– AI æ¯æ—¥è¯„çº§ï¼ˆåŒä¸€å¤©å†…ç¼“å­˜ç»“æœï¼‰"""
    today = datetime.now().strftime("%Y-%m-%d")

    # æ£€æŸ¥æ¯æ—¥ç¼“å­˜
    if today in _rating_cache:
        return {"code": 0, "data": _rating_cache[today]}

    # å¹¶å‘è·å–æ•°æ®
    stock_data, news_list, kline_data = await asyncio.gather(
        fetch_stock_data(),
        fetch_news(),
        fetch_kline_data(period="day", count=30),
    )

    # æ²¡æœ‰ API Key æ—¶è¿”å›é»˜è®¤ä¸­æ€§è¯„çº§
    if not LLM_API_KEY:
        fallback = {
            "date": today,
            "rating": "ä¸­æ€§",
            "score": 50,
            "summary": "æœªé…ç½® AI å¤§æ¨¡å‹ API Keyï¼Œæ— æ³•ç”Ÿæˆæ™ºèƒ½è¯„çº§ã€‚è¯·åœ¨ .env ä¸­é…ç½® LLM_API_KEY åé‡è¯•ã€‚",
            "factors": {
                "technical": "æ— æ³•åˆ†æ",
                "fundamental": "æ— æ³•åˆ†æ",
                "sentiment": "æ— æ³•åˆ†æ",
            },
        }
        _rating_cache[today] = fallback
        return {"code": 0, "data": fallback}

    # æ„å»ºè¯„çº§ Prompt
    news_text = "\n".join(
        [f"- {n['title']}ï¼ˆ{n['source']}ï¼‰" for n in news_list[:12]]
    ) or "æš‚æ— æœ€æ–°æ–°é—»"

    kline_summary = ""
    if kline_data:
        recent = kline_data[-10:]
        kline_summary = "è¿‘10ä¸ªäº¤æ˜“æ—¥è¡Œæƒ…:\n"
        for k in recent:
            kline_summary += (
                f"  {k['date']}: å¼€{k['open']} æ”¶{k['close']} "
                f"é«˜{k['high']} ä½{k['low']}\n"
            )

    prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±æ¸¯è‚¡åˆ†æå¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹è…¾è®¯æ§è‚¡(00700.HK)æœ€æ–°æ•°æ®ï¼Œç»™å‡ºä»Šæ—¥æŠ•èµ„è¯„çº§ã€‚

## å½“å‰è‚¡ç¥¨æ•°æ®
- å½“å‰ä»·æ ¼: {stock_data.get('current_price', '--')} HKD
- æ¶¨è·Œå¹…: {stock_data.get('change_percent', '--')}%
- ä»Šå¼€: {stock_data.get('open', '--')} æœ€é«˜: {stock_data.get('high', '--')} æœ€ä½: {stock_data.get('low', '--')}
- æˆäº¤é‡: {stock_data.get('volume', '--')}  æˆäº¤é¢: {stock_data.get('turnover', '--')}
- PE: {stock_data.get('pe_ratio', '--')}  PB: {stock_data.get('pb_ratio', '--')}
- å¸‚å€¼: {stock_data.get('market_cap', '--')}äº¿
- æ¢æ‰‹ç‡: {stock_data.get('turnover_rate', '--')}%  æŒ¯å¹…: {stock_data.get('amplitude', '--')}%
- 52å‘¨é«˜: {stock_data.get('52w_high', '--')} 52å‘¨ä½: {stock_data.get('52w_low', '--')}

{kline_summary}

## æœ€æ–°æ–°é—»
{news_text}

## è¯„çº§è¦æ±‚
è¯·ä¸¥æ ¼ä»¥å¦‚ä¸‹ JSON æ ¼å¼è¿”å›ï¼ˆä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹ï¼Œä»…è¿”å› JSONï¼‰ï¼š
{{
  "rating": "å¼ºçƒˆæ¨è/æ¨è/ä¸­æ€§/è°¨æ…/å›é¿ï¼ˆäº”é€‰ä¸€ï¼‰",
  "score": 0-100çš„æ•´æ•°è¯„åˆ†,
  "summary": "ä¸€å¥è¯è¯„çº§ç†ç”±ï¼ˆ30å­—ä»¥å†…ï¼‰",
  "factors": {{
    "technical": "æŠ€æœ¯é¢ä¸€å¥è¯åˆ¤æ–­ï¼ˆ20å­—ä»¥å†…ï¼‰",
    "fundamental": "åŸºæœ¬é¢ä¸€å¥è¯åˆ¤æ–­ï¼ˆ20å­—ä»¥å†…ï¼‰",
    "sentiment": "æ¶ˆæ¯é¢ä¸€å¥è¯åˆ¤æ–­ï¼ˆ20å­—ä»¥å†…ï¼‰"
  }}
}}

è¯„åˆ†å‚è€ƒ: å¼ºçƒˆæ¨è 80-100, æ¨è 60-79, ä¸­æ€§ 40-59, è°¨æ… 20-39, å›é¿ 0-19
è¯„çº§æ—¥æœŸ: {today}"""

    try:
        result_text = ""
        req_body = {
            "model": LLM_MODEL,
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±æ¸¯è‚¡åˆ†æå¸ˆã€‚è¯·ä¸¥æ ¼æŒ‰è¦æ±‚çš„JSONæ ¼å¼è¿”å›è¯„çº§ç»“æœï¼Œä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ã€‚"},
                {"role": "user", "content": prompt},
            ],
            "temperature": 0.3,
            "max_tokens": 2000,
        }
        if "2.5" in LLM_MODEL:
            req_body["reasoning_effort"] = "low"

        async with httpx.AsyncClient(timeout=httpx.Timeout(120, connect=15)) as client:
            resp = await client.post(
                f"{LLM_BASE_URL}/chat/completions",
                headers={
                    "Authorization": f"Bearer {LLM_API_KEY}",
                    "Content-Type": "application/json",
                },
                json=req_body,
            )
            if resp.status_code == 200:
                body = resp.json()
                print(f"[Rating] APIå“åº”keys: {list(body.keys())}")
                msg = body.get("choices", [{}])[0].get("message", {})
                content = msg.get("content") or ""
                # Gemini 2.5 thinking æ¨¡å‹å¯èƒ½åœ¨ parts ä¸­è¿”å›
                if not content and "parts" in msg:
                    for part in msg["parts"]:
                        if isinstance(part, dict) and part.get("text"):
                            content = part["text"]
                            break
                result_text = content.strip()
                print(f"[Rating] æå–åˆ°å†…å®¹é•¿åº¦: {len(result_text)}, å‰100å­—: {result_text[:100]}")
            else:
                err_body = resp.text[:300]
                print(f"[Rating] APIè¿”å› {resp.status_code}: {err_body}")
                raise Exception(f"API HTTP {resp.status_code}")

        if not result_text:
            raise Exception("AIè¿”å›å†…å®¹ä¸ºç©º")

        # è§£æ JSONï¼ˆå®¹é”™å¤„ç†ï¼šå»æ‰å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°ï¼‰
        cleaned = result_text
        if "```" in cleaned:
            # æå–ç¬¬ä¸€ä¸ª ``` å’Œæœ€åä¸€ä¸ª ``` ä¹‹é—´çš„å†…å®¹
            import re
            json_match = re.search(r"```(?:json)?\s*\n?(.*?)```", cleaned, re.DOTALL)
            if json_match:
                cleaned = json_match.group(1)
        cleaned = cleaned.strip()

        rating_data = json.loads(cleaned)

        # æ ¡éªŒå¿…è¦å­—æ®µ
        valid_ratings = ["å¼ºçƒˆæ¨è", "æ¨è", "ä¸­æ€§", "è°¨æ…", "å›é¿"]
        if rating_data.get("rating") not in valid_ratings:
            rating_data["rating"] = "ä¸­æ€§"
        score = int(rating_data.get("score", 50))
        rating_data["score"] = max(0, min(100, score))
        rating_data["date"] = today

        if "factors" not in rating_data:
            rating_data["factors"] = {
                "technical": "--",
                "fundamental": "--",
                "sentiment": "--",
            }

        _rating_cache[today] = rating_data
        return {"code": 0, "data": rating_data}

    except Exception as e:
        print(f"[Rating] è¯„çº§å¼‚å¸¸: {e}")
        fallback = {
            "date": today,
            "rating": "ä¸­æ€§",
            "score": 50,
            "summary": f"AI è¯„çº§ç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚",
            "factors": {
                "technical": "--",
                "fundamental": "--",
                "sentiment": "--",
            },
        }
        return {"code": 0, "data": fallback}


# ---------------------------------------------------------------------------
# é€šç”¨æµå¼ LLM è°ƒç”¨
# ---------------------------------------------------------------------------
async def _stream_llm(system_prompt: str, user_prompt: str, max_tokens: int = 2000):
    """é€šç”¨æµå¼ LLM è°ƒç”¨ï¼Œyield æ–‡æœ¬ chunk"""
    if not LLM_API_KEY:
        yield "> âš ï¸ æœªé…ç½® LLM_API_KEYï¼Œæ— æ³•è°ƒç”¨ AI æ¨¡å‹ã€‚\n"
        return

    # æ„å»ºè¯·æ±‚ä½“
    req_body = {
        "model": LLM_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "stream": True,
        "temperature": 0.7,
        "max_tokens": max_tokens,
    }
    # Gemini 2.5 thinking æ¨¡å‹: ç”¨ low é™åˆ¶æ€è€ƒ tokenï¼ŒæŠŠæ›´å¤šé…é¢ç»™å®é™…è¾“å‡º
    if "2.5" in LLM_MODEL:
        req_body["max_tokens"] = max(max_tokens, 8000)
        req_body["reasoning_effort"] = "low"

    try:
        async with httpx.AsyncClient(timeout=httpx.Timeout(300, connect=15)) as client:
            async with client.stream(
                "POST",
                f"{LLM_BASE_URL}/chat/completions",
                headers={
                    "Authorization": f"Bearer {LLM_API_KEY}",
                    "Content-Type": "application/json",
                },
                json=req_body,
            ) as response:
                if response.status_code != 200:
                    body = await response.aread()
                    error_msg = ""
                    try:
                        err = json.loads(body)
                        if isinstance(err, list) and err:
                            err = err[0]
                        error_msg = err.get("error", {}).get("message", "")[:200]
                    except Exception:
                        error_msg = body.decode("utf-8", errors="ignore")[:200]
                    print(f"[LLM] APIè¿”å› {response.status_code}: {error_msg}")
                    yield f"\n> âš ï¸ AIæ¨¡å‹è°ƒç”¨å¤±è´¥ï¼ˆHTTP {response.status_code}ï¼‰ï¼š{error_msg}\n"
                    return

                async for line in response.aiter_lines():
                    if line.startswith("data: "):
                        data_str = line[6:]
                        if data_str.strip() == "[DONE]":
                            break
                        try:
                            chunk = json.loads(data_str)
                            delta = chunk["choices"][0]["delta"]
                            if "content" in delta:
                                yield delta["content"]
                        except (json.JSONDecodeError, KeyError, IndexError):
                            continue
    except Exception as e:
        print(f"[LLM] è°ƒç”¨å¼‚å¸¸: {e}")
        yield f"\n> âš ï¸ AIè°ƒç”¨å¼‚å¸¸ï¼š{str(e)[:100]}\n"


def _sse_wrap(generator):
    """åŒ…è£… async generator ä¸º SSE StreamingResponse"""
    async def event_stream():
        async for chunk in generator:
            yield f"data: {json.dumps({'content': chunk}, ensure_ascii=False)}\n\n"
        yield "data: [DONE]\n\n"

    return StreamingResponse(
        event_stream(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )


# ---------------------------------------------------------------------------
# æ–°é—»æ‘˜è¦ API
# ---------------------------------------------------------------------------
@app.get("/api/summary")
async def get_summary():
    """AI æ€»ç»“æœ€æ–°æ–°é—»èµ„è®¯ï¼ˆæµå¼ï¼‰"""
    stock_data, news_list = await asyncio.gather(
        fetch_stock_data(),
        fetch_news(),
    )

    news_text = "\n".join(
        [f"{i+1}. [{n.get('lang','zh')=='en' and 'EN' or 'CN'}] {n['title']}ï¼ˆ{n['source']}ï¼‰"
         for i, n in enumerate(news_list[:20])]
    ) or "æš‚æ— æ–°é—»"

    prompt = f"""è¯·å¯¹ä»¥ä¸‹è…¾è®¯æ§è‚¡ï¼ˆ00700.HKï¼‰ç›¸å…³æ–°é—»è¿›è¡Œä¸“ä¸šæ€»ç»“åˆ†æã€‚

## å½“å‰è‚¡ä»·ä¿¡æ¯
- ä»·æ ¼: {stock_data.get('current_price', '--')} HKD
- æ¶¨è·Œ: {stock_data.get('change', '--')} ({stock_data.get('change_percent', '--')}%)

## æœ€æ–°æ–°é—»åˆ—è¡¨
{news_text}

## è¯·è¾“å‡º:
1. **æ–°é—»è¦ç‚¹æ€»ç»“**ï¼ˆ3-5ä¸ªæ ¸å¿ƒè¦ç‚¹ï¼Œæ¯ä¸ª1-2å¥è¯ï¼‰
2. **å¸‚åœºæƒ…ç»ªåˆ¤æ–­**ï¼ˆåå¤š/åç©º/ä¸­æ€§ï¼Œå¹¶è¯´æ˜åŸå› ï¼‰
3. **å…³é”®å…³æ³¨ç‚¹**ï¼ˆæœªæ¥éœ€è¦è·Ÿè¸ªçš„é‡ç‚¹äº‹ä»¶æˆ–æ•°æ®ï¼‰

è¦æ±‚ï¼šç®€æ´ç²¾ç‚¼ï¼Œè¦ç‚¹æ˜ç¡®ï¼Œä¸­æ–‡è¾“å‡ºã€‚å¦‚æœæœ‰è‹±æ–‡æ–°é—»è¯·ç¿»è¯‘æ€»ç»“ã€‚
åˆ†ææ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}"""

    return _sse_wrap(_stream_llm(
        system_prompt="ä½ æ˜¯ä¸€ä½èµ„æ·±è´¢ç»æ–°é—»ç¼–è¾‘å’Œæ¸¯è‚¡åˆ†æå¸ˆï¼Œæ“…é•¿ä»æµ·é‡æ–°é—»ä¸­æç‚¼æ ¸å¿ƒä¿¡æ¯ã€‚è¯·ç”¨ç®€æ´ä¸“ä¸šçš„é£æ ¼æ€»ç»“ã€‚",
        user_prompt=prompt,
        max_tokens=1500,
    ))


# ---------------------------------------------------------------------------
# è‡ªå®šä¹‰æé—® API
# ---------------------------------------------------------------------------
class ChatRequest(BaseModel):
    prompt: str


@app.post("/api/chat")
async def post_chat(req: ChatRequest):
    """ç”¨æˆ·è‡ªå®šä¹‰æç¤ºè¯åˆ†æï¼ˆæµå¼ï¼‰"""
    stock_data, news_list, kline_data = await asyncio.gather(
        fetch_stock_data(),
        fetch_news(),
        fetch_kline_data(),
    )

    news_brief = "\n".join(
        [f"- {n['title']}" for n in news_list[:10]]
    ) or "æš‚æ— "

    kline_brief = ""
    if kline_data:
        recent = kline_data[-5:]
        kline_brief = "\n".join(
            [f"  {k['date']}: å¼€{k['open']} æ”¶{k['close']} é«˜{k['high']} ä½{k['low']}" for k in recent]
        )

    context = f"""## è…¾è®¯æ§è‚¡(00700.HK) å½“å‰æ•°æ®
- ä»·æ ¼: {stock_data.get('current_price', '--')} HKD
- æ¶¨è·Œ: {stock_data.get('change', '--')} ({stock_data.get('change_percent', '--')}%)
- ä»Šå¼€: {stock_data.get('open', '--')} æœ€é«˜: {stock_data.get('high', '--')} æœ€ä½: {stock_data.get('low', '--')}
- æˆäº¤é‡: {stock_data.get('volume', '--')} æˆäº¤é¢: {stock_data.get('turnover', '--')}
- PE: {stock_data.get('pe_ratio', '--')} å¸‚å€¼: {stock_data.get('market_cap', '--')}

## è¿‘5æ—¥è¡Œæƒ…
{kline_brief or 'æš‚æ— '}

## æœ€æ–°æ–°é—»
{news_brief}

---

## ç”¨æˆ·çš„é—®é¢˜
{req.prompt}
"""

    return _sse_wrap(_stream_llm(
        system_prompt="ä½ æ˜¯ä¸€ä½èµ„æ·±æ¸¯è‚¡åˆ†æå¸ˆå’ŒAIæŠ•èµ„é¡¾é—®ã€‚ç”¨æˆ·ä¼šåŸºäºè…¾è®¯æ§è‚¡çš„å®æ—¶æ•°æ®å‘ä½ æé—®ï¼Œè¯·ç»™å‡ºä¸“ä¸šã€å®¢è§‚çš„å›ç­”ã€‚ä½¿ç”¨Markdownæ ¼å¼è¾“å‡ºã€‚",
        user_prompt=context,
        max_tokens=3000,
    ))


# é™æ€æ–‡ä»¶å’Œé¦–é¡µ
app.mount("/static", StaticFiles(directory="static"), name="static")


@app.get("/")
async def index():
    return FileResponse("static/index.html")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run("app:app", host="0.0.0.0", port=8000, reload=True)
